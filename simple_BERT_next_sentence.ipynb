{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT for next sentence prediction\n",
    "\n",
    "The hardest part of this is making sure you've got the python packages you need installed. You'll need to install ```torch``` and ```transformers,``` and as usual with python, you may run into compatibility issues.\n",
    "\n",
    "All I can say to help there is \"google the error message\"?\n",
    "\n",
    "But once you've got the packages installed it's easy.\n",
    "\n",
    "First we load everything and get it ready to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48521a0e94474e69ba3f44697b8ebc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484734935e14476f8bd5862233d98080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329805ec256a4adcabe4abbe64e24909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae62bd57fea48378c11caafae70ef4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f421a4170c884c20a513f68df53e8d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print('built tokenizer')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "print('built model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then here's a function to do next sentence prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD FUNCTION\n",
    "\n",
    "def old_get_logits(firstsentence, secondsentence):\n",
    "    global tokenizer, model\n",
    "\n",
    "    encoding = tokenizer.encode_plus(firstsentence, secondsentence, return_tensors = 'pt', max_seq_length = 255)\n",
    "    loss, logits = model(**encoding, next_sentence_label=torch.LongTensor([1]))\n",
    "\n",
    "    return loss, logits\n",
    "\n",
    "# NEW FUNCTIONS\n",
    "\n",
    "def get_raw_output(firstsentence, secondsentence, tokenizer, model):\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(firstsentence, secondsentence, return_tensors = 'pt', padding = False)\n",
    "    result = model(**encoding)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_logits(firstsentence, secondsentence, tokenizer, model):\n",
    "\n",
    "    encoding = tokenizer.encode_plus(firstsentence, secondsentence, return_tensors = 'pt', padding = False)\n",
    "    result_object = model(**encoding)\n",
    "    \n",
    "    logits = result_object['logits'].tolist()[0]\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes to this function are necessitated because [HuggingFace has implemented a new way to wrap \"model outputs\"](https://huggingface.co/transformers/main_classes/output.html) since I originally wrote this.\n",
    "\n",
    "You used to get two numeric results. Now you get a NextSentencePredictorOutput, which in turn wraps the results as PyTorch Tensors. Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"At the store I bought bananas and milk.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextSentencePredictorOutput(loss=None, logits=tensor([[ 6.2713, -6.1164]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = get_raw_output(firstsentence, secondsentence, tokenizer, model)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading [the HuggingFace documentation](https://huggingface.co/transformers/main_classes/output.html) and [Googling Stack Overflow](https://stackoverflow.com/questions/53903373/convert-pytorch-tensor-to-python-list) I was able to write the new get_logits function, which unpacks those objects to get numbers we can deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.271258354187012, -6.116359233856201]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits(firstsentence, secondsentence, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation between logits and probability makes my head hurt to explain, so I'm just going to [point at Wikipedia.](https://en.wikipedia.org/wiki/Logit)\n",
    "\n",
    "But for a quick and dirty approach I wrote this function which *loosely* translates BERT's logits output into a probability for the sequence. Also checked [this blog post](https://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f) to confirm that the probability of \"yes, this is the next sentence\" is associated with the first logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_probability(firstsent, secondsent):\n",
    "    '''\n",
    "    \n",
    "    :param logits: a tensor produced by BERT\n",
    "    :return: probability of the first category after softmax\n",
    "    '''\n",
    "    global tokenizer, model\n",
    "    \n",
    "    logits = get_logits(firstsent, secondsent, tokenizer, model)\n",
    "    \n",
    "    poslogit = logits[0]\n",
    "    neglogit = logits[1]\n",
    "\n",
    "    pospart = math.pow(2.72, poslogit)\n",
    "    negpart = math.pow(2.72, neglogit)\n",
    "\n",
    "    posprob = pospart / (pospart + negpart)\n",
    "\n",
    "    return round(posprob, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999996"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"At the store I bought bananas and milk.\"\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now we can see that BERT considers that a pretty probable sequence. Let's try a less probable sequence.\n",
    "\n",
    "We'll use the same first sentence about walking to the store, and for our second sentence\n",
    "\n",
    "    Psychedelics are a hallucinogenic class of psychoactive drug whose primary effect is to trigger non-ordinary states of consciousness and psychedelic experiences via serotonin 2A receptor agonism.\n",
    "    \n",
    "Which is from Wikipedia on \"psychedelic drug.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"Psychedelics are a hallucinogenic class of psychoactive drug whose primary effect is to trigger non-ordinary states of consciousness and psychedelic experiences via serotonin 2A receptor agonism.\"\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a much less probable sequence! Let's try a slightly weaker non-sequitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextSentencePredictorOutput(loss=None, logits=tensor([[-3.4202,  6.4734]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05549650796189104"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentence = \"I was walking to the store one day to buy groceries.\"\n",
    "secondsentence = \"Everything is closed due to the pandemic.\"\n",
    "get_probability(firstsentence, secondsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that probability is slightly higher. Still unlikely. But not *totally* improbable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
